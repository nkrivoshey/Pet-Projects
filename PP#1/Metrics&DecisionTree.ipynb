{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67d049b-e27e-4a93-8ccf-2e503835cfef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**В данном файле мною реализованы classification/regression metrics & процедура алгоритма DecisionTree. Таким образом, этот файл включает не \"from sklearn.... import ....\", а как работает тот или иной алгоритм \"под капотом\".**\n",
    "* метрики классификации: AP, Precision, Recall, F_beta, Accuracy, Cross-entropy \n",
    "* метрики регрессии: MSE, MAE, MSLE, Quantile standart error\n",
    "* Использование алгоритма DecisionTree для обучения и предикта\n",
    "\n",
    "> В дополнении хочется отметить, что алгоритм дерева решений был написан в .py, чтобы не загромождать текущий файл. Его можно переписать под конкретную задачу или добавить/убрать гиперпараметры, которые реализованы там."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c391faf-464e-4a15-ae44-a59ccf25e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортирование необходимых библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18cc78ba-b658-40e3-b10f-8de51aa9f5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Импортирование класса из файла DecisionTreeClass, чтобы решение выглядело более компактно\n",
    "# нужно, чтобы файл .py был в одной директории с юпитер ноутбуком\n",
    "from DecisionTreeClass import DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6869b81-e98f-4c42-9451-bfa6c3fd9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализация метрик классификации\n",
    "def average_precision(y_true, y_pred):\n",
    "    y_true_sorted, y_pred_sorted = zip(*sorted(zip(y_true, y_pred), key=lambda x: x[1], reverse=True))\n",
    "    cumsum = np.cumsum(y_true_sorted)\n",
    "    precision = cumsum / (np.arange(len(y_true_sorted)) + 1)\n",
    "    recall = cumsum / np.sum(y_true)\n",
    "    ap = np.sum(precision[:-1] * (recall[1:] - recall[:-1]))\n",
    "    return ap\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    \n",
    "    denominator = true_positives + false_positives\n",
    "    if denominator == 0:\n",
    "        return 0.0  \n",
    "    else:\n",
    "        return true_positives / denominator\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    denominator = true_positives + false_negatives\n",
    "    if denominator == 0:\n",
    "        return 0.0  \n",
    "    else:\n",
    "        return true_positives / denominator\n",
    "\n",
    "def f_beta(y_true, y_pred, beta=1):\n",
    "    precision_value = precision(y_true, y_pred)\n",
    "    recall_value = recall(y_true, y_pred)\n",
    "    \n",
    "    denominator = (beta**2 * precision_value) + recall_value\n",
    "    if denominator == 0:\n",
    "        return 0.0  \n",
    "    else:\n",
    "        return (1 + beta**2) * (precision_value * recall_value) / denominator\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    total_samples = len(y_true)\n",
    "    return correct_predictions / total_samples\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    cross_entropy_value = - np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return cross_entropy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff44cbd-1673-4bc5-abbc-864abd6d3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализация метрик регрессии\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def mean_squared_log_error(y_true, y_pred):\n",
    "    epsilon = 1e-10\n",
    "    if any(y <= 0 for y in y_true) or any(y <= 0 for y in y_pred):\n",
    "        return 0\n",
    "    \n",
    "    return np.mean((np.log(y_true + epsilon) - np.log(y_pred + epsilon))**2)\n",
    "\n",
    "def quantile_standard_error(y_true, y_pred, q=0.5):\n",
    "    quantile_error = np.abs(y_true - y_pred) - q * (y_true - y_pred)\n",
    "    return np.mean(np.maximum(quantile_error, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446ee9f-8390-400d-a699-c564cda886ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для создания датасетов с заданными уже \"начальными\" параметрами, которые можно изменять\n",
    "def generate_classification(num_datasets=10, samples=1000, features=20, random_state=None):\n",
    "    datasets = []\n",
    "    for _ in range(num_datasets):\n",
    "        X, y = make_classification(n_samples=samples, n_features=features, random_state=random_state)\n",
    "        datasets.append((X, y))\n",
    "    return datasets\n",
    "\n",
    "def generate_regression(num_datasets=10, samples=1000, features=20, random_state=None):\n",
    "    datasets = []\n",
    "    for _ in range(num_datasets):\n",
    "        X, y = make_regression(n_samples=samples, n_features=features, random_state=random_state)\n",
    "        datasets.append((X, y))\n",
    "    return datasets\n",
    "\n",
    "# Функция для разделения выборки ( train, test, val)\n",
    "test_size = ...\n",
    "validation_size = ...\n",
    "\n",
    "def split_dataset(X, y, test_size = test_size, validation_size = validation_size, random_state=None):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(test_size + validation_size), random_state=random_state)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=validation_size/(test_size + validation_size), random_state=random_state)\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "# Обучение и оценка моделей classification\n",
    "def train_and_evaluate_classifier(X_train, X_val, y_train, y_val, criterion, splitter, leaf,mx_features):\n",
    "    model = DecisionTree(criterion = criterion , splitter = splitter , min_samples_leaf = leaf, max_features = mx_features, task = 'classification')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    ap = average_precision(y_val, y_val_pred)\n",
    "    accuracy_score = accuracy(y_val, y_val_pred)\n",
    "    precision_score = precision(y_val, y_val_pred)\n",
    "    recall_score = recall(y_val, y_val_pred)\n",
    "    fbeta_score = f_beta(y_val, y_val_pred, beta=1)\n",
    "    cross_entropy_score = cross_entropy(y_val, y_val_pred)\n",
    "    return ap, accuracy_score, precision_score, recall_score, fbeta_score, cross_entropy_score\n",
    "\n",
    "# Обучение и оценка моделей regression\n",
    "def train_and_evaluate_regressor(X_train, X_val, y_train, y_val, criterion, splitter, leaf,mx_features):\n",
    "    model = DecisionTree(criterion = criterion , splitter = splitter , min_samples_leaf = leaf, max_features = mx_features,task = 'regression')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    mse = mean_squared_error(y_val, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    msle = mean_squared_log_error(y_val, y_val_pred)\n",
    "    qse = quantile_standard_error(y_val, y_val_pred)\n",
    "    return mse, mae, msle, qse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806fcd4-817d-4bce-b0d7-1528ed89f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем 'n' датасетов для обучения и предикта нашего \"одинокого дерева\"\n",
    "\n",
    "# args для генерации(можно записать в dict и передать в функцию через **args или через знак \"=\" )\n",
    "num_datasets = ...\n",
    "samples = ...\n",
    "features = ...\n",
    "random_state = ... #optional\n",
    "\n",
    "\n",
    "classification_datasets = generate_classification(num_datasets = num_datasets) # Пример \n",
    "regression_datasets = generate_regression(num_datasets = num_datasets) # Пример \n",
    "\n",
    "# При обучении и подсчете метрик реализован обычный цикл для наглядности работы алгоритма\n",
    "# С данными функциями можно реализовать более сложные конструкции, которые будут требоваться в вашем случае\n",
    "\n",
    "# Классификация(можно изменить значения гиперпарам)\n",
    "cl_metrics_criterion = [\"gini\", \"entropy\"]\n",
    "cl_metrics_splitter = ['best', 'random']\n",
    "cl_metrics_mx_features = [None,'auto','log2','sqrt']\n",
    "min_samples_leaf = [2,3,4]\n",
    "cl_results = []\n",
    "\n",
    "for dataset in classification_datasets:\n",
    "    X, y = dataset\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = split_dataset(X, y, random_state=42)\n",
    "\n",
    "    for criterion, splitter,mx_features ,leaf in zip(cl_metrics_criterion,cl_metrics_splitter,cl_metrics_mx_features,min_samples_leaf):\n",
    "        ap, accuracy_score, precision_score, recall_score, fbeta_score, cross_entropy_score = train_and_evaluate_classifier(X_train, X_val, y_train, y_val, criterion, splitter, leaf, mx_features)\n",
    "        cl_results.append((criterion, splitter, mx_features ,leaf , ap, accuracy_score, precision_score, recall_score, fbeta_score, cross_entropy_score))\n",
    "\n",
    "# Регрессия(можно изменить значения гиперпарам)\n",
    "reg_metrics_criterion = [\"mse\"]\n",
    "reg_metrics_splitter = ['best', 'random']\n",
    "reg_metrics_mx_features = [None,'auto','log2','sqrt']\n",
    "reg_results = []\n",
    "\n",
    "for dataset in regression_datasets:\n",
    "    X, y = dataset\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = split_dataset(X, y, random_state=42)\n",
    "\n",
    "    for criterion, splitter,mx_features ,leaf in zip(reg_metrics_criterion,reg_metrics_splitter,reg_metrics_mx_features,min_samples_leaf):\n",
    "        mse, mae, msle, qse = train_and_evaluate_regressor(X_train, X_val, y_train, y_val, criterion, splitter, leaf, mx_features)\n",
    "        reg_results.append((criterion, splitter, mx_features ,leaf, mse, mae, msle, qse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef8e1f7-e20c-4080-9ae7-3e33c8612538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим значения метрик после обучения\n",
    "classification_metrics_indices = [4, 5, 6, 7, 8]\n",
    "regression_metrics_indices = [4, 5, 6, 7]\n",
    "\n",
    "# Функция для агрегации метрик\n",
    "def aggregate_metrics(metrics):\n",
    "    return np.mean(metrics)\n",
    "\n",
    "sorted_classification_results = sorted(cl_results, key=lambda x: aggregate_metrics([x[i] for i in classification_metrics_indices]), reverse=True)\n",
    "sorted_regression_results = sorted(reg_results, key=lambda x: aggregate_metrics([x[i] for i in regression_metrics_indices]))\n",
    "\n",
    "\n",
    "print(\"Результаты классификации:\")\n",
    "for result in sorted_classification_results:\n",
    "    print(result)\n",
    "\n",
    "print(\"\\nРезультаты регрессии:\")\n",
    "for result in sorted_regression_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa342e-6877-4627-9642-c072d521bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбор гиперпараметров с лучшим скором\n",
    "best_classification_criterion = sorted_classification_results[0]\n",
    "print(\"Лучшие гиперпараметры модели для классификации:\", best_classification_criterion)\n",
    "\n",
    "# Выбор лучшего критерия для регрессии\n",
    "best_regression_criterion = sorted_regression_results[-1]\n",
    "print(\"Лучшие гиперпараметры модели для регрессии:\", best_regression_criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
